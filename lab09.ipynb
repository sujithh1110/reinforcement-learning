{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdtQL6d8b21iNbsnZ+0Gpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujithh1110/reinforcement-learning/blob/main/lab09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPqep9Xum6zb",
        "outputId": "baea6d78-a41f-41fc-ae0a-5dbef5bbbe15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install torch gymnasium[classic-control] numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Q45mgJLnrTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PPO (continuous actions) — PyTorch implementation\n",
        "Works with gymnasium (Pendulum-v1 by default).\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_   # numpy >= 2.0 compat\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "# --------------------------\n",
        "# Actor-Critic network\n",
        "# --------------------------\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.mu_head = nn.Linear(hidden_size, action_dim)\n",
        "        # log std as parameter (one per action dim)\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "        self.value_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        mu = self.mu_head(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        value = self.value_head(x)\n",
        "        return mu, std, value.squeeze(-1)\n",
        "\n",
        "# --------------------------\n",
        "# Helper functions\n",
        "# --------------------------\n",
        "def discount_cumsum(x, discount):\n",
        "    # reversed discounted cumulative sums\n",
        "    return np.array([sum((discount**i) * x[i + t] for i in range(len(x) - t)) for t in range(len(x))])\n",
        "\n",
        "# --------------------------\n",
        "# PPO agent / training\n",
        "# --------------------------\n",
        "class PPO:\n",
        "    def __init__(self,\n",
        "                 env_name=\"Pendulum-v1\",\n",
        "                 hidden_size=256,\n",
        "                 lr=3e-4,\n",
        "                 gamma=0.99,\n",
        "                 lam=0.95,\n",
        "                 clip_eps=0.2,\n",
        "                 epochs=10,\n",
        "                 minibatch_size=64,\n",
        "                 rollout_steps=2048,\n",
        "                 policy_epochs=10,\n",
        "                 value_coef=0.5,\n",
        "                 ent_coef=0.0,\n",
        "                 max_grad_norm=0.5,\n",
        "                 device=None):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        self.device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "\n",
        "        self.obs_dim = self.env.observation_space.shape[0]\n",
        "        self.act_dim = self.env.action_space.shape[0]\n",
        "        self.act_low = self.env.action_space.low\n",
        "        self.act_high = self.env.action_space.high\n",
        "\n",
        "        self.model = ActorCritic(self.obs_dim, self.act_dim, hidden_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.clip_eps = clip_eps\n",
        "        self.epochs = epochs\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.rollout_steps = rollout_steps\n",
        "        self.policy_epochs = policy_epochs\n",
        "        self.value_coef = value_coef\n",
        "        self.ent_coef = ent_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "    def scale_action(self, raw_action):\n",
        "        # raw_action from Normal distribution — treat as unbounded; we squash with tanh and scale to env range\n",
        "        # But here we sample directly in action space and clip\n",
        "        return np.clip(raw_action, self.act_low, self.act_high)\n",
        "\n",
        "    def collect_rollout(self):\n",
        "        obs_buf = []\n",
        "        act_buf = []\n",
        "        logp_buf = []\n",
        "        rew_buf = []\n",
        "        val_buf = []\n",
        "        done_buf = []\n",
        "\n",
        "        obs, _ = self.env.reset()\n",
        "        for _ in range(self.rollout_steps):\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                mu, std, value = self.model(obs_tensor)\n",
        "                dist = Normal(mu, std)\n",
        "                action = dist.sample()\n",
        "                logp = dist.log_prob(action).sum(axis=-1)\n",
        "            action_np = action.cpu().numpy().squeeze(0)\n",
        "            clipped_action = self.scale_action(action_np)\n",
        "            next_obs, reward, terminated, truncated, _ = self.env.step(clipped_action)\n",
        "            done = float(terminated or truncated)\n",
        "\n",
        "            obs_buf.append(obs)\n",
        "            act_buf.append(action_np)\n",
        "            logp_buf.append(logp.cpu().item())\n",
        "            rew_buf.append(reward)\n",
        "            val_buf.append(value.cpu().item())\n",
        "            done_buf.append(done)\n",
        "\n",
        "            obs = next_obs\n",
        "            if done:\n",
        "                obs, _ = self.env.reset()\n",
        "\n",
        "        # get last value for bootstrap\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            _, _, last_val = self.model(obs_tensor)\n",
        "            last_val = last_val.cpu().item()\n",
        "\n",
        "        # convert to numpy arrays\n",
        "        return {\n",
        "            'obs': np.array(obs_buf, dtype=np.float32),\n",
        "            'acts': np.array(act_buf, dtype=np.float32),\n",
        "            'logp': np.array(logp_buf, dtype=np.float32),\n",
        "            'rews': np.array(rew_buf, dtype=np.float32),\n",
        "            'vals': np.array(val_buf, dtype=np.float32),\n",
        "            'dones': np.array(done_buf, dtype=np.float32),\n",
        "            'last_val': last_val\n",
        "        }\n",
        "\n",
        "    def compute_gae(self, rews, vals, dones, last_val):\n",
        "        # Generalized advantage estimation\n",
        "        T = len(rews)\n",
        "        adv = np.zeros(T, dtype=np.float32)\n",
        "        last_gae = 0.0\n",
        "        for t in reversed(range(T)):\n",
        "            if t == T - 1:\n",
        "                next_value = last_val\n",
        "                next_non_terminal = 1.0 - dones[t]\n",
        "            else:\n",
        "                next_value = vals[t+1]\n",
        "                next_non_terminal = 1.0 - dones[t]\n",
        "            delta = rews[t] + self.gamma * next_value * next_non_terminal - vals[t]\n",
        "            last_gae = delta + self.gamma * self.lam * next_non_terminal * last_gae\n",
        "            adv[t] = last_gae\n",
        "        returns = adv + vals\n",
        "        return adv, returns\n",
        "\n",
        "    def ppo_update(self, batch, batch_size):\n",
        "        obs = torch.tensor(batch['obs'], dtype=torch.float32, device=self.device)\n",
        "        acts = torch.tensor(batch['acts'], dtype=torch.float32, device=self.device)\n",
        "        old_logp = torch.tensor(batch['logp'], dtype=torch.float32, device=self.device)\n",
        "        adv = torch.tensor(batch['adv'], dtype=torch.float32, device=self.device)\n",
        "        ret = torch.tensor(batch['ret'], dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # normalize advantage\n",
        "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "\n",
        "        N = obs.shape[0]\n",
        "        idxs = np.arange(N)\n",
        "        for _ in range(self.policy_epochs):\n",
        "            np.random.shuffle(idxs)\n",
        "            for start in range(0, N, self.minibatch_size):\n",
        "                mb_idx = idxs[start:start + self.minibatch_size]\n",
        "                mb_obs = obs[mb_idx]\n",
        "                mb_acts = acts[mb_idx]\n",
        "                mb_old_logp = old_logp[mb_idx]\n",
        "                mb_adv = adv[mb_idx]\n",
        "                mb_ret = ret[mb_idx]\n",
        "\n",
        "                mu, std, value = self.model(mb_obs)\n",
        "                dist = Normal(mu, std)\n",
        "                mb_logp = dist.log_prob(mb_acts).sum(axis=-1)\n",
        "                entropy = dist.entropy().sum(axis=-1).mean()\n",
        "\n",
        "                ratio = torch.exp(mb_logp - mb_old_logp)\n",
        "                surr1 = ratio * mb_adv\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * mb_adv\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                critic_loss = nn.MSELoss()(value, mb_ret)\n",
        "                loss = actor_loss + self.value_coef * critic_loss - self.ent_coef * entropy\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "\n",
        "    def train(self, total_updates=200):\n",
        "        print(f\"Training on {self.env_name} with device={self.device}\")\n",
        "        start_time = time.time()\n",
        "        for update in range(1, total_updates + 1):\n",
        "            batch = self.collect_rollout()\n",
        "            adv, ret = self.compute_gae(batch['rews'], batch['vals'], batch['dones'], batch['last_val'])\n",
        "            batch['adv'] = adv\n",
        "            batch['ret'] = ret\n",
        "\n",
        "            self.ppo_update(batch, self.minibatch_size)\n",
        "\n",
        "            # simple logging: average return estimate over rollout\n",
        "            avg_ret = np.mean(ret)\n",
        "            avg_reward = np.mean(batch['rews'])\n",
        "            if update % 5 == 0 or update == 1:\n",
        "                print(f\"Update {update}/{total_updates} | avg_rollout_reward {avg_reward:.3f} | avg_ret {avg_ret:.3f}\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Training finished in {total_time:.1f}s. Saving model to ppo_continuous.pth\")\n",
        "        torch.save(self.model.state_dict(), \"ppo_continuous.pth\")\n",
        "\n",
        "    def evaluate(self, episodes=5, render=False):\n",
        "        env = gym.make(self.env_name, render_mode=\"human\" if render else None)\n",
        "        for ep in range(episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            total = 0.0\n",
        "            steps = 0\n",
        "            while True:\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                with torch.no_grad():\n",
        "                    mu, std, _ = self.model(obs_t)\n",
        "                action = mu.cpu().numpy().squeeze(0)\n",
        "                action = self.scale_action(action)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                total += reward\n",
        "                steps += 1\n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            print(f\"Eval ep {ep+1}: reward={total:.3f} steps={steps}\")\n",
        "        env.close()\n",
        "\n",
        "# --------------------------\n",
        "# Run\n",
        "# --------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # hyperparameters you can tweak\n",
        "    ppo = PPO(env_name=\"Pendulum-v1\",\n",
        "              hidden_size=256,\n",
        "              lr=3e-4,\n",
        "              gamma=0.99,\n",
        "              lam=0.95,\n",
        "              clip_eps=0.2,\n",
        "              epochs=10,\n",
        "              minibatch_size=64,\n",
        "              rollout_steps=2048,\n",
        "              policy_epochs=10,\n",
        "              value_coef=0.5,\n",
        "              ent_coef=0.0)\n",
        "\n",
        "    # train (updates). Each update collects 'rollout_steps' environment steps.\n",
        "    ppo.train(total_updates=50)\n",
        "\n",
        "    # evaluate learned policy\n",
        "    ppo.evaluate(episodes=3, render=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xItNOHqqBFRk",
        "outputId": "f76fdef6-74d6-4e04-820d-3d8482811d6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Pendulum-v1 with device=cpu\n",
            "Update 1/50 | avg_rollout_reward -6.851 | avg_ret -106.073\n",
            "Update 5/50 | avg_rollout_reward -5.738 | avg_ret -170.517\n",
            "Update 10/50 | avg_rollout_reward -6.604 | avg_ret -278.488\n",
            "Update 15/50 | avg_rollout_reward -6.424 | avg_ret -345.139\n",
            "Update 20/50 | avg_rollout_reward -6.170 | avg_ret -337.140\n",
            "Update 25/50 | avg_rollout_reward -5.048 | avg_ret -322.204\n",
            "Update 30/50 | avg_rollout_reward -6.745 | avg_ret -409.416\n",
            "Update 35/50 | avg_rollout_reward -5.826 | avg_ret -356.974\n",
            "Update 40/50 | avg_rollout_reward -6.481 | avg_ret -378.089\n",
            "Update 45/50 | avg_rollout_reward -6.779 | avg_ret -436.620\n",
            "Update 50/50 | avg_rollout_reward -6.003 | avg_ret -382.338\n",
            "Training finished in 107.2s. Saving model to ppo_continuous.pth\n",
            "Eval ep 1: reward=-971.618 steps=200\n",
            "Eval ep 2: reward=-1141.698 steps=200\n",
            "Eval ep 3: reward=-1124.850 steps=200\n"
          ]
        }
      ]
    }
  ]
}