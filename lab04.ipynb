{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM/x9Qy+sjYmi5a1K1BFzT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujithh1110/reinforcement-learning/blob/main/lab04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rp5BuuwzIko",
        "outputId": "b3fdd775-1099-45b4-e393-2e6af5cebec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space (states): 16, Action space: 4\n",
            "\n",
            "TD(0) Value Function under equiprobable-random policy (first 10 states):\n",
            "[0.0089 0.007  0.0121 0.0082 0.0111 0.     0.0192 0.     0.0214 0.0634]\n",
            "Average return of random policy over 1000 episodes: 0.013\n",
            "\n",
            "Average return of SARSA-learned greedy policy over 2000 episodes: 0.728\n",
            "\n",
            "Greedy action (first 10 states): ['←', '↑', '←', '↑', '←', '←', '←', '←', '↑', '↓']\n",
            "\n",
            "Sample Q-values (first 3 states):\n",
            "State 0: [0.406 0.368 0.37  0.38 ]\n",
            "State 1: [0.259 0.164 0.133 0.344]\n",
            "State 2: [0.295 0.263 0.244 0.258]\n"
          ]
        }
      ],
      "source": [
        "# frozenlake_td0_sarsa.py\n",
        "# Gymnasium + NumPy only. TD(0) policy evaluation and SARSA control on FrozenLake-v1.\n",
        "# Works on CPU; no extras required.\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "def make_random_policy(nS: int, nA: int) -> np.ndarray:\n",
        "    \"\"\"Equiprobable random policy: π(a|s) = 1/nA.\"\"\"\n",
        "    policy = np.ones((nS, nA), dtype=np.float64) / nA\n",
        "    return policy\n",
        "\n",
        "def epsilon_greedy_probs(Q: np.ndarray, state: int, epsilon: float) -> np.ndarray:\n",
        "    \"\"\"Return action probabilities for ε-greedy over Q(state, ·).\"\"\"\n",
        "    nA = Q.shape[1]\n",
        "    probs = np.ones(nA, dtype=np.float64) * (epsilon / nA)\n",
        "    best_a = np.argmax(Q[state])\n",
        "    probs[best_a] += 1.0 - epsilon\n",
        "    return probs\n",
        "\n",
        "def run_episode(env, policy_probs: np.ndarray, max_steps: int, rng: np.random.Generator) -> Tuple[float, int]:\n",
        "    \"\"\"Run one episode following a (stochastic) policy, return total reward and steps.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    total_r = 0.0\n",
        "    steps = 0\n",
        "    for _ in range(max_steps):\n",
        "        a = rng.choice(env.action_space.n, p=policy_probs[obs])\n",
        "        next_obs, r, terminated, truncated, _ = env.step(a)\n",
        "        total_r += r\n",
        "        steps += 1\n",
        "        obs = next_obs\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return total_r, steps\n",
        "\n",
        "# ----------------------------\n",
        "# TD(0) Policy Evaluation\n",
        "# ----------------------------\n",
        "def td0_policy_evaluation(\n",
        "    env,\n",
        "    policy_probs: np.ndarray,\n",
        "    alpha: float = 0.1,\n",
        "    gamma: float = 0.99,\n",
        "    num_episodes: int = 20_000,\n",
        "    max_steps: int = 200,\n",
        "    seed: int = 0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    On-policy TD(0) state-value estimation under the given (stochastic) policy.\n",
        "    V(s) <- V(s) + α [ R + γ V(s') - V(s) ]\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    nS = env.observation_space.n\n",
        "    V = np.zeros(nS, dtype=np.float64)\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        s, _ = env.reset(seed=None)\n",
        "        for t in range(max_steps):\n",
        "            a = rng.choice(env.action_space.n, p=policy_probs[s])\n",
        "            s_next, r, terminated, truncated, _ = env.step(a)\n",
        "            target = r + (0.0 if (terminated or truncated) else gamma * V[s_next])\n",
        "            V[s] += alpha * (target - V[s])\n",
        "            s = s_next\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "    return V\n",
        "\n",
        "# ----------------------------\n",
        "# SARSA Control (on-policy)\n",
        "# ----------------------------\n",
        "def sarsa_control(\n",
        "    env,\n",
        "    alpha: float = 0.1,\n",
        "    gamma: float = 0.99,\n",
        "    epsilon: float = 1.0,\n",
        "    epsilon_min: float = 0.05,\n",
        "    epsilon_decay: float = 0.9995,\n",
        "    num_episodes: int = 50_000,\n",
        "    max_steps: int = 200,\n",
        "    seed: int = 0,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    On-policy SARSA to learn Q and the ε-greedy policy derived from Q.\n",
        "    Update: Q[s,a] <- Q[s,a] + α [ r + γ Q[s', a'] - Q[s,a] ]\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        # (Optional) seed only first reset for determinism of runs, not every episode.\n",
        "        s, _ = env.reset()\n",
        "        # ε-greedy action\n",
        "        a = rng.choice(nA, p=epsilon_greedy_probs(Q, s, epsilon))\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            s_next, r, terminated, truncated, _ = env.step(a)\n",
        "            if terminated or truncated:\n",
        "                td_target = r  # terminal next state's value = 0\n",
        "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
        "                break\n",
        "\n",
        "            # Choose next action a' using ε-greedy on next state\n",
        "            a_next = rng.choice(nA, p=epsilon_greedy_probs(Q, s_next, epsilon))\n",
        "\n",
        "            td_target = r + gamma * Q[s_next, a_next]\n",
        "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
        "\n",
        "            s, a = s_next, a_next\n",
        "\n",
        "        # Decay epsilon after each episode (on-policy)\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # Derive greedy policy from Q\n",
        "    greedy_policy = np.zeros_like(Q)\n",
        "    best_actions = np.argmax(Q, axis=1)\n",
        "    greedy_policy[np.arange(nS), best_actions] = 1.0\n",
        "    return Q, greedy_policy\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation helpers\n",
        "# ----------------------------\n",
        "def evaluate_policy(\n",
        "    env,\n",
        "    policy_probs: np.ndarray,\n",
        "    episodes: int = 1_000,\n",
        "    max_steps: int = 200,\n",
        "    seed: int = 123,\n",
        ") -> float:\n",
        "    \"\"\"Average episodic return following a (stochastic) policy.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    total = 0.0\n",
        "    for _ in range(episodes):\n",
        "        r, _ = run_episode(env, policy_probs, max_steps, rng)\n",
        "        total += r\n",
        "    return total / episodes\n",
        "\n",
        "def value_from_Q(Q: np.ndarray, policy_probs: np.ndarray, gamma: float = 0.99, tol: float = 1e-10) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    (Optional) Compute V^π from Q by V(s) = Σ_a π(a|s) Q(s,a). For deterministic policies, this equals max_a Q(s,a).\n",
        "    \"\"\"\n",
        "    return (policy_probs * Q).sum(axis=1)\n",
        "\n",
        "# ----------------------------\n",
        "# Main demo\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # You can flip is_slippery to False for an easier, deterministic environment.\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    print(f\"Observation space (states): {nS}, Action space: {nA}\")\n",
        "\n",
        "    # ----- TD(0) policy evaluation on a random policy -----\n",
        "    random_policy = make_random_policy(nS, nA)\n",
        "    V_td0 = td0_policy_evaluation(\n",
        "        env,\n",
        "        random_policy,\n",
        "        alpha=0.05,\n",
        "        gamma=0.99,\n",
        "        num_episodes=25_000,\n",
        "        max_steps=200,\n",
        "        seed=42,\n",
        "    )\n",
        "    print(\"\\nTD(0) Value Function under equiprobable-random policy (first 10 states):\")\n",
        "    print(np.round(V_td0[:10], 4))\n",
        "\n",
        "    # Evaluate how good that random policy is (it’s usually poor on FrozenLake)\n",
        "    rand_return = evaluate_policy(env, random_policy, episodes=1_000, max_steps=200)\n",
        "    print(f\"Average return of random policy over 1000 episodes: {rand_return:.3f}\")\n",
        "\n",
        "    # ----- SARSA control to learn a policy -----\n",
        "    Q, greedy_policy = sarsa_control(\n",
        "        env,\n",
        "        alpha=0.1,\n",
        "        gamma=0.99,\n",
        "        epsilon=1.0,\n",
        "        epsilon_min=0.05,\n",
        "        epsilon_decay=0.999,  # slower decay for more exploration on slippery variant\n",
        "        num_episodes=60_000,\n",
        "        max_steps=200,\n",
        "        seed=123,\n",
        "    )\n",
        "\n",
        "    # Evaluate learned (greedy) policy\n",
        "    learned_return = evaluate_policy(env, greedy_policy, episodes=2_000, max_steps=200)\n",
        "    print(f\"\\nAverage return of SARSA-learned greedy policy over 2000 episodes: {learned_return:.3f}\")\n",
        "\n",
        "    # Show first 10 states' greedy actions and their Q-values\n",
        "    actions_map = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
        "    best_actions = np.argmax(Q, axis=1)\n",
        "    print(\"\\nGreedy action (first 10 states):\", [actions_map[a] for a in best_actions[:10]])\n",
        "    print(\"\\nSample Q-values (first 3 states):\")\n",
        "    for s in range(min(3, nS)):\n",
        "        print(f\"State {s}: {np.round(Q[s], 3)}\")"
      ]
    }
  ]
}