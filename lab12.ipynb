{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCgM89p/tNnXktZe+KVYZp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujithh1110/reinforcement-learning/blob/main/lab12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDXhBwAswi0M",
        "outputId": "55d5b53a-6337-43ca-e9fc-7b4e4c59d3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training with epsilon_greedy ===\n",
            "Episode 0: avg reward (last 100) = 0.00\n",
            "Episode 500: avg reward (last 100) = 0.35\n",
            "Episode 1000: avg reward (last 100) = 0.46\n",
            "Episode 1500: avg reward (last 100) = 0.79\n",
            "\n",
            "=== Training with softmax ===\n",
            "Episode 0: avg reward (last 100) = 0.00\n",
            "Episode 500: avg reward (last 100) = 0.05\n",
            "Episode 1000: avg reward (last 100) = 0.06\n",
            "Episode 1500: avg reward (last 100) = 0.05\n",
            "\n",
            "=== Training with random ===\n",
            "Episode 0: avg reward (last 100) = 0.00\n",
            "Episode 500: avg reward (last 100) = 0.00\n",
            "Episode 1000: avg reward (last 100) = 0.01\n",
            "Episode 1500: avg reward (last 100) = 0.01\n",
            "\n",
            "=== FINAL RESULTS (avg last 200 episodes) ===\n",
            "Epsilon Greedy : 0.84\n",
            "Softmax        : 0.08\n",
            "Random         : 0.03\n"
          ]
        }
      ],
      "source": [
        "#@title Fast Exploration Strategies Comparison (Tabular Q-Learning)\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# Environment (very fast)\n",
        "# -----------------------------\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)  # deterministic version\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# -----------------------------\n",
        "# Q-Learning Parameters\n",
        "# -----------------------------\n",
        "EPISODES = 2000\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.99\n",
        "\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 0.999\n",
        "\n",
        "TEMP = 1.0   # softmax temperature\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Strategies\n",
        "# -----------------------------\n",
        "def epsilon_greedy(q_table, state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, n_actions - 1)\n",
        "    return np.argmax(q_table[state])\n",
        "\n",
        "\n",
        "def softmax_action(q_table, state, temp):\n",
        "    prefs = q_table[state] / temp\n",
        "    probs = np.exp(prefs) / np.sum(np.exp(prefs))\n",
        "    return np.random.choice(n_actions, p=probs)\n",
        "\n",
        "\n",
        "def random_action(q_table, state):\n",
        "    return random.randint(0, n_actions - 1)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop for each strategy\n",
        "# -----------------------------\n",
        "def train(strategy_name):\n",
        "    print(f\"\\n=== Training with {strategy_name} ===\")\n",
        "    q_table = np.zeros((n_states, n_actions))\n",
        "    epsilon = EPS_START\n",
        "    rewards = []\n",
        "\n",
        "    for ep in range(EPISODES):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for _ in range(100):\n",
        "            # Choose action\n",
        "            if strategy_name == \"epsilon_greedy\":\n",
        "                action = epsilon_greedy(q_table, state, epsilon)\n",
        "\n",
        "            elif strategy_name == \"softmax\":\n",
        "                action = softmax_action(q_table, state, TEMP)\n",
        "\n",
        "            elif strategy_name == \"random\":\n",
        "                action = random_action(q_table, state)\n",
        "\n",
        "            # Step\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            # Update Q-value\n",
        "            q_table[state, action] += ALPHA * (\n",
        "                reward + GAMMA * np.max(q_table[next_state]) - q_table[state, action]\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # epsilon decay (only for epsilon greedy)\n",
        "        if strategy_name == \"epsilon_greedy\":\n",
        "            epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if ep % 500 == 0:\n",
        "            print(f\"Episode {ep}: avg reward (last 100) = {np.mean(rewards[-100:]):.2f}\")\n",
        "\n",
        "    return q_table, rewards\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Train all 3 strategies\n",
        "# -----------------------------\n",
        "q_eps, r_eps = train(\"epsilon_greedy\")\n",
        "q_soft, r_soft = train(\"softmax\")\n",
        "q_rand, r_rand = train(\"random\")\n",
        "\n",
        "print(\"\\n=== FINAL RESULTS (avg last 200 episodes) ===\")\n",
        "print(f\"Epsilon Greedy : {np.mean(r_eps[-200:]):.2f}\")\n",
        "print(f\"Softmax        : {np.mean(r_soft[-200:]):.2f}\")\n",
        "print(f\"Random         : {np.mean(r_rand[-200:]):.2f}\")\n"
      ]
    }
  ]
}