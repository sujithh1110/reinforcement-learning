{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvRI71RZ03coDtjEQNKHJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujithh1110/reinforcement-learning/blob/main/lab11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw1tsZ8ku66x",
        "outputId": "51ae8da4-045e-4500-e3a9-40c42e9fb0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 0.28.1 does not provide the extra 'accept-rom-data'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting expert demonstrations...\n",
            "Collected 8569 state-action pairs in 200 episodes (0.1s).\n",
            "Using device: cpu\n",
            "Training behavioral cloning model...\n",
            "Epoch 01 | Train loss 0.5408 acc 0.8097 | Val loss 0.2839 acc 0.9357\n",
            "Epoch 02 | Train loss 0.1802 acc 0.9520 | Val loss 0.1301 acc 0.9556\n",
            "Epoch 03 | Train loss 0.1113 acc 0.9624 | Val loss 0.1103 acc 0.9556\n",
            "Epoch 04 | Train loss 0.0867 acc 0.9704 | Val loss 0.0859 acc 0.9685\n",
            "Epoch 05 | Train loss 0.0719 acc 0.9746 | Val loss 0.0706 acc 0.9708\n",
            "Epoch 06 | Train loss 0.0627 acc 0.9776 | Val loss 0.0636 acc 0.9708\n",
            "Epoch 07 | Train loss 0.0579 acc 0.9795 | Val loss 0.0482 acc 0.9907\n",
            "Epoch 08 | Train loss 0.0481 acc 0.9830 | Val loss 0.0630 acc 0.9731\n",
            "Epoch 09 | Train loss 0.0440 acc 0.9856 | Val loss 0.0462 acc 0.9848\n",
            "Epoch 10 | Train loss 0.0415 acc 0.9848 | Val loss 0.0374 acc 0.9895\n",
            "Epoch 11 | Train loss 0.0380 acc 0.9860 | Val loss 0.0359 acc 0.9883\n",
            "Epoch 12 | Train loss 0.0343 acc 0.9886 | Val loss 0.0315 acc 0.9895\n",
            "Epoch 13 | Train loss 0.0324 acc 0.9881 | Val loss 0.0311 acc 0.9907\n",
            "Epoch 14 | Train loss 0.0300 acc 0.9881 | Val loss 0.0591 acc 0.9731\n",
            "Epoch 15 | Train loss 0.0316 acc 0.9885 | Val loss 0.0290 acc 0.9918\n",
            "Epoch 16 | Train loss 0.0291 acc 0.9885 | Val loss 0.0338 acc 0.9813\n",
            "Epoch 17 | Train loss 0.0250 acc 0.9908 | Val loss 0.0253 acc 0.9907\n",
            "Epoch 18 | Train loss 0.0234 acc 0.9912 | Val loss 0.0276 acc 0.9883\n",
            "Epoch 19 | Train loss 0.0263 acc 0.9895 | Val loss 0.0272 acc 0.9883\n",
            "Epoch 20 | Train loss 0.0226 acc 0.9913 | Val loss 0.0211 acc 0.9930\n",
            "Epoch 21 | Train loss 0.0237 acc 0.9903 | Val loss 0.0218 acc 0.9930\n",
            "Epoch 22 | Train loss 0.0214 acc 0.9924 | Val loss 0.0265 acc 0.9883\n",
            "Epoch 23 | Train loss 0.0214 acc 0.9921 | Val loss 0.0198 acc 0.9918\n",
            "Epoch 24 | Train loss 0.0193 acc 0.9935 | Val loss 0.0178 acc 0.9942\n",
            "Epoch 25 | Train loss 0.0187 acc 0.9933 | Val loss 0.0193 acc 0.9942\n",
            "\n",
            "Evaluating expert policy...\n",
            "Expert: mean return = 41.48, std = 9.74, max = 62.00\n",
            "Evaluating cloned policy...\n",
            "Cloned: mean return = 45.92, std = 10.13, max = 65.00\n",
            "\n",
            "Summary:\n",
            "  Expert avg return over 50 eps: 41.48\n",
            "  Cloned avg return over 50 eps: 45.92\n",
            "\n",
            "Model saved to bc_cartpole_model.pth\n",
            "\n",
            "Sample states -> expert_action, cloned_action\n",
            "[ 0.018 -0.189 -0.113  0.161] -> expert: 0, cloned: 0\n",
            "[-0.109 -1.574  0.115  1.858] -> expert: 1, cloned: 1\n",
            "[-0.166 -0.734  0.169  0.874] -> expert: 1, cloned: 1\n",
            "[-0.084 -0.768  0.109  1.016] -> expert: 1, cloned: 1\n",
            "[-0.042  0.199  0.107 -0.299] -> expert: 1, cloned: 1\n",
            "[-0.099 -1.012  0.15   1.364] -> expert: 1, cloned: 1\n",
            "[ 0.028 -0.239 -0.021  0.246] -> expert: 0, cloned: 0\n",
            "[ 0.041 -0.806 -0.114  0.908] -> expert: 0, cloned: 0\n",
            "\n",
            "Done. If you want to visualize the cloned policy run:\n",
            "  - set render=True in run_policy(cloned_policy, n_episodes=1, render=True)\n",
            "  - or use gym's video recorder in Colab (requires extra setup).\n"
          ]
        }
      ],
      "source": [
        "#@title Behavioral Cloning for CartPole (PyTorch) — Run in Google Colab\n",
        "# If you run in local env with gym already installed you can skip installs.\n",
        "\n",
        "# --- Install required packages (safe to run in Colab) ---\n",
        "!pip install --quiet gymnasium==0.28.1 gymnasium[accept-rom-data] torch torchvision\n",
        "\n",
        "# --- Imports ---\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "# --- Reproducibility ---\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "ENV_NAME = \"CartPole-v1\"\n",
        "NUM_EXPERT_EPISODES = 200          # number of episodes to collect from expert\n",
        "MAX_STEPS_PER_EPISODE = 500\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "NUM_EPOCHS = 25\n",
        "HIDDEN_SIZE = 64\n",
        "VALIDATION_SPLIT = 0.1\n",
        "EVAL_EPISODES = 50\n",
        "\n",
        "# --- Expert policy ---\n",
        "# Simple heuristic expert: push in direction of pole angle\n",
        "def expert_policy(obs):\n",
        "    # obs: [cart_pos, cart_vel, pole_angle, pole_vel]\n",
        "    pole_angle = obs[2]\n",
        "    action = 1 if pole_angle > 0 else 0\n",
        "    return action\n",
        "\n",
        "# --- Data collection from expert ---\n",
        "env = gym.make(ENV_NAME)\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "observations = []\n",
        "actions = []\n",
        "\n",
        "print(\"Collecting expert demonstrations...\")\n",
        "start_time = time.time()\n",
        "for ep in range(NUM_EXPERT_EPISODES):\n",
        "    o, _ = env.reset(seed=SEED + ep)\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done and steps < MAX_STEPS_PER_EPISODE:\n",
        "        a = expert_policy(o)\n",
        "        observations.append(o.copy())\n",
        "        actions.append(a)\n",
        "        o, reward, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "        steps += 1\n",
        "end_time = time.time()\n",
        "print(f\"Collected {len(observations)} state-action pairs in {NUM_EXPERT_EPISODES} episodes ({end_time-start_time:.1f}s).\")\n",
        "\n",
        "observations = np.array(observations, dtype=np.float32)\n",
        "actions = np.array(actions, dtype=np.int64)\n",
        "\n",
        "# Shuffle data\n",
        "perm = np.random.permutation(len(observations))\n",
        "observations = observations[perm]\n",
        "actions = actions[perm]\n",
        "\n",
        "# Split into train / val\n",
        "val_size = int(len(observations) * VALIDATION_SPLIT)\n",
        "if val_size == 0:\n",
        "    train_obs, val_obs = observations, None\n",
        "    train_act, val_act = actions, None\n",
        "else:\n",
        "    train_obs, val_obs = observations[val_size:], observations[:val_size]\n",
        "    train_act, val_act = actions[val_size:], actions[:val_size]\n",
        "\n",
        "# Create DataLoaders\n",
        "train_ds = TensorDataset(torch.from_numpy(train_obs), torch.from_numpy(train_act))\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "if val_obs is not None:\n",
        "    val_ds = TensorDataset(torch.from_numpy(val_obs), torch.from_numpy(val_act))\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "else:\n",
        "    val_loader = None\n",
        "\n",
        "# --- Model (simple MLP classifier) ---\n",
        "class BCModel(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_size, n_actions):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = BCModel(obs_dim, HIDDEN_SIZE, action_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# --- Training loop ---\n",
        "print(\"Training behavioral cloning model...\")\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss.item()) * xb.size(0)\n",
        "        total += xb.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "    train_loss = total_loss / total\n",
        "    train_acc = correct / total\n",
        "    # Validation\n",
        "    if val_loader is not None:\n",
        "        model.eval()\n",
        "        vtotal = 0\n",
        "        vcorrect = 0\n",
        "        vloss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device); yb = yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                vloss += float(loss.item()) * xb.size(0)\n",
        "                vtotal += xb.size(0)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                vcorrect += (preds == yb).sum().item()\n",
        "        val_loss = vloss / vtotal\n",
        "        val_acc = vcorrect / vtotal\n",
        "        print(f\"Epoch {epoch:02d} | Train loss {train_loss:.4f} acc {train_acc:.4f} | Val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch:02d} | Train loss {train_loss:.4f} acc {train_acc:.4f}\")\n",
        "\n",
        "# --- Evaluation of cloned policy ---\n",
        "def run_policy(policy_fn, n_episodes=20, render=False):\n",
        "    env_eval = gym.make(ENV_NAME)\n",
        "    returns = []\n",
        "    lengths = []\n",
        "    for ep in range(n_episodes):\n",
        "        o, _ = env_eval.reset(seed=SEED + 1000 + ep)\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        steps = 0\n",
        "        while not done and steps < MAX_STEPS_PER_EPISODE:\n",
        "            a = policy_fn(o)\n",
        "            o, r, terminated, truncated, _ = env_eval.step(int(a))\n",
        "            done = terminated or truncated\n",
        "            total_reward += r\n",
        "            steps += 1\n",
        "            if render:\n",
        "                env_eval.render()\n",
        "        returns.append(total_reward)\n",
        "        lengths.append(steps)\n",
        "    env_eval.close()\n",
        "    return np.array(returns), np.array(lengths)\n",
        "\n",
        "# Expert evaluation\n",
        "print(\"\\nEvaluating expert policy...\")\n",
        "expert_returns, expert_lengths = run_policy(expert_policy, n_episodes=EVAL_EPISODES)\n",
        "print(f\"Expert: mean return = {expert_returns.mean():.2f}, std = {expert_returns.std():.2f}, max = {expert_returns.max():.2f}\")\n",
        "\n",
        "# Cloned policy using the trained network\n",
        "def cloned_policy(obs):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(device)\n",
        "        logits = model(x)\n",
        "        action = int(torch.argmax(logits, dim=1).item())\n",
        "    return action\n",
        "\n",
        "print(\"Evaluating cloned policy...\")\n",
        "cloned_returns, cloned_lengths = run_policy(cloned_policy, n_episodes=EVAL_EPISODES)\n",
        "print(f\"Cloned: mean return = {cloned_returns.mean():.2f}, std = {cloned_returns.std():.2f}, max = {cloned_returns.max():.2f}\")\n",
        "\n",
        "# Basic comparison\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"  Expert avg return over {EVAL_EPISODES} eps: {expert_returns.mean():.2f}\")\n",
        "print(f\"  Cloned avg return over {EVAL_EPISODES} eps: {cloned_returns.mean():.2f}\")\n",
        "\n",
        "# --- Save model (optional) ---\n",
        "torch.save(model.state_dict(), \"bc_cartpole_model.pth\")\n",
        "print(\"\\nModel saved to bc_cartpole_model.pth\")\n",
        "\n",
        "# Show a few sample predictions (for debugging/visual check)\n",
        "print(\"\\nSample states -> expert_action, cloned_action\")\n",
        "for i in range(8):\n",
        "    s = observations[i]\n",
        "    e_a = expert_policy(s)\n",
        "    c_a = cloned_policy(s)\n",
        "    print(f\"{np.round(s,3)} -> expert: {e_a}, cloned: {c_a}\")\n",
        "\n",
        "print(\"\\nDone. If you want to visualize the cloned policy run:\\n\"\n",
        "      \"  - set render=True in run_policy(cloned_policy, n_episodes=1, render=True)\\n\"\n",
        "      \"  - or use gym's video recorder in Colab (requires extra setup).\")\n"
      ]
    }
  ]
}